{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saishanakar/FML/blob/main/tokenization_with_3_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokeniztion using python split function\n"
      ],
      "metadata": {
        "id": "E7ccQYsg8zU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''orem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.'''\n",
        "tokens=text.split()\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "O8xJVA_59ENa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66eb7b2d-f9e9-4c0e-b799-6b242b42d518"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['orem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry.', 'Lorem', 'Ipsum', 'has', 'been', 'the', \"industry's\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s,', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries,', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting,', 'remaining', 'essentially', 'unchanged.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages,', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum.']\n",
            "91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text='''Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.'''\n",
        "sentences=text.split('.')\n",
        "print(sentences)\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "id": "vQTXZvH2-OFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20a5e364-a34b-41c1-96d0-204e51fd32d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem Ipsum is simply dummy text of the printing and typesetting industry', \" Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book\", ' It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged', ' It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum', '']\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization using Regular Expressions (RegEx)"
      ],
      "metadata": {
        "id": "3IWVNL55zZIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "tZKNmB6T-pi6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=re.findall(\"[\\w']+\",text)\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "tH2UsOjQ-tSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97391bdd-bd81-4f3f-e97f-a1e661b3ce30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', 'Lorem', 'Ipsum', 'has', 'been', 'the', \"industry's\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', 'remaining', 'essentially', 'unchanged', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum']\n",
            "91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = re.compile('[.?!] ').split(text)\n",
        "print(sentences)\n",
        "print(\"No.of sentences : \", len(sentences))"
      ],
      "metadata": {
        "id": "IU2ttRvsAmBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a81952a9-3b9a-413e-af76-d83f9bc46fc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem Ipsum is simply dummy text of the printing and typesetting industry', \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book\", 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged', 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.']\n",
            "No.of sentences :  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization using nltk\n"
      ],
      "metadata": {
        "id": "Q6BYgzw5CNND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "id": "vOK92y88CSKr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1da11e3c-1fd1-49e1-ca27-79341541b23b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "_1OH6pm_EygV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4373eca-600e-476d-8000-14e4ddbad69b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "print(\"No.of tokens : \", len(tokens))\n"
      ],
      "metadata": {
        "id": "1mqUD-SPE1hv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1a947c-ba4f-4cb9-d51b-3a3f48c4ce3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.', 'Lorem', 'Ipsum', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s', ',', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen', 'book', '.', 'It', 'has', 'survived', 'not', 'only', 'five', 'centuries', ',', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'typesetting', ',', 'remaining', 'essentially', 'unchanged', '.', 'It', 'was', 'popularised', 'in', 'the', '1960s', 'with', 'the', 'release', 'of', 'Letraset', 'sheets', 'containing', 'Lorem', 'Ipsum', 'passages', ',', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'Aldus', 'PageMaker', 'including', 'versions', 'of', 'Lorem', 'Ipsum', '.']\n",
            "No.of tokens :  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "print(\"No.of sentences : \", len(sentences))"
      ],
      "metadata": {
        "id": "wvPilmX2E4wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bcd8a14-0c26-491f-b12b-afc2a63dd910"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem Ipsum is simply dummy text of the printing and typesetting industry.', \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\", 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.', 'It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.']\n",
            "No.of sentences :  4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n"
      ],
      "metadata": {
        "id": "ufP-zDXME7oN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"troubling\"))"
      ],
      "metadata": {
        "id": "9Wo8HmSDE-dG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13bf6a2f-fb70-4574-aaf5-01fd95c59e4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "troubl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a function ht taks a sentence and gives stemmed sentence\n"
      ],
      "metadata": {
        "id": "J191VPQTFWAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "\n",
        "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def stemSentence(sentence):\n",
        "    token_words=word_tokenize(sentence)\n",
        "    print(token_words)\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "x=stemSentence(sentence)\n",
        "print(\"Sentence after stemming :\", x)"
      ],
      "metadata": {
        "id": "MC33rlzFFBXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1063d8cf-6aa4-48f7-a4d5-d10bfd27161d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pythoners', 'are', 'very', 'intelligent', 'and', 'work', 'very', 'pythonly', 'and', 'now', 'they', 'are', 'pythoning', 'their', 'way', 'to', 'success', '.']\n",
            "Sentence after stemming : python are veri intellig and work veri pythonli and now they are python their way to success . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comvert given sentence to lemmatized sentence\n"
      ],
      "metadata": {
        "id": "ee7SPCV3FgCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "SXkTd_WpFPow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317480ec-102a-48dc-99d6-5eb7660e5a48"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
        "punctuations=\"?:!.,;\"\n",
        "token_words = nltk.word_tokenize(sentence)\n",
        "print(token_words)\n",
        "\n",
        "lemma_sentence=[]\n",
        "for word in token_words:\n",
        "  lemma_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
        "  lemma_sentence.append(\" \")\n",
        "\n",
        "print(\"lemmas of tokens: \", ''.join(lemma_sentence))"
      ],
      "metadata": {
        "id": "p3oSTsdbFEaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49083873-7be8-4407-8616-ac7c70f0862e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', '.', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun', '.']\n",
            "lemmas of tokens:  He wa running and eating at same time . He ha bad habit of swimming after playing long hour in the Sun . \n"
          ]
        }
      ]
    }
  ]
}